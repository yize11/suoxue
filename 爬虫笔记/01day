网页的三大特性?:
1.每一个网页都有自己唯一的URL地址(统一资源定位符)
2.网页都是通过HTML(超文本)来展示数据的
3.网页是通过http/https(超文本传输协议)来传输html的数据


爬虫最基本的步骤?:
1.寻找目标url,发起请求
2.获取请求的响应结果,分析响应结果
3.从响应结果中提取数据
	a.第一部分,从网页中提取的目标数据
	b.如果存在新的url地址,则提取,继续发起请求
爬虫结束:所有的目标url全部请求完毕,爬虫结束

数据的用途：
1.爬取数据,写自己的网站
2.搜索引擎
3.购物助手
4.日常数据基本分析(知乎数据冰山专栏)


做爬虫并不只有python可以完成的:
java   php  c/c++  switch ...

java:是python写爬虫的最大的竞争对手,java的发展周期长,生态圈都比较完善,也有很多第三方库的支持,java的代码量比较大,开发的成本比较高,后期维护也比较繁琐

php:php曾经被叫做世界上最好的语音(一般用来后端的),也可以用来写爬虫,但是对多任务的支持不太好,爬虫对效率要求比较高,所有一般不适用php写爬虫


c/c++:比较偏向于底层语音,代码的运行效率高,学习非常高,代码成型比较慢


python:代码简单易懂,并且第三方的库也有很多,python自带的urllib网路呢请求模块,requests网路请求模块,网路解析xpath，BeautifulSoup4,pyquery等等,还有成熟高效稳定的爬虫框架scrapy(pyspider)等等,并且还支持分布式爬虫(scrapy-redis)框架

爬虫的分类(通用爬虫,聚焦爬虫):
通用爬虫:是搜索引擎的重要组成部分
作用和目的:尽可能全的将互联网上所有的网页,下载本地,通过分词,去噪等进行处理,处理后进行数据的持久化,然后提取检索系统


获取数据步骤):
	a:获取一些种子url,放入待爬去队列
	b:从待爬队列中取出url发起请求,将获取的响应结果进行处理,之后存入本地,然后将已爬取的url,放入已经获取队列
	c.从响应结果中获取外链(url),将url放入待爬取队列中


DNS服务:将域转换为ip的技术


搜索引擎如何获取新网站的url:
1.向搜素引擎直接提交url地址
2.通过网页的外链
3.跟DNS服务商合作,新网站注册的域对应的网站将会迅速被搜素引擎抓取


搜素引擎的排名:
1.根据用户的访问量和网站的流量进行排名
2.竞价排名:根据价钱进行网站的排名

搜索引擎需要遵守robot协议:
是一个规范,网站通过robot协议告诉搜素引擎哪些页面可以爬取,哪些页面不可以爬取



搜索引擎的缺点:
1.只能够获取简单的文件数据,大型的二进制数据(音频,视频)都不能够获取
2.搜索引擎搜索的结果千篇一律,没有办法根据特定的用获取特定的数据
3.搜索引擎搜索结果99%并没用


由于搜索引擎的缺点,产生了聚焦爬虫
聚焦爬虫:是面向主题,面向需求的爬虫,只获取跟需求相关的数据


OSI七层协议的目的:实现不同的系统互联之间的数据通讯实现数据的传输.
应用层　表示层　会话层　传输层　网络层　数据链络层　物理层

应用层:http/https
传输层:TCP/UDP
TCP:网络传输协议,面向连接的,长连接,传输的数据流,确保数据的安全和完整性,但是数据传输的效率低
UDP:网络传输协议,是非面向连接的,短连接,传输的是数据包,传输数据是不安全的,可能会造成数据丢失,传输速度快

http(超文本传输协议,端口号是80):
实现从网络传输草文本数据到本地浏览器的传送协议
https(端口号443):是http的安全版本,在http的基础上添加了一个SSL(安全套接字层)层,用于web端的安全传送,在传输层对网络连接进行加密
1.构建了一个安全的数据传输通道,
2.保证网站的真实性和有效性


常见的请求状态码:
200:请求成功
300:重定向
301:永久重定向
302:临时重定向

4xx:客户端请求错误
400:请求错误,服务器无法解析
401:未授权,没有进行身份验证
403:服务器拒绝访问
404:访问页面不存在
405:请求方式不允许
408:请求超时





























